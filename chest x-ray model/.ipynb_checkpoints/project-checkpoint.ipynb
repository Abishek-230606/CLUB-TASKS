{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bcf265-14c8-45e6-8908-8f85c08206fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# AIML Chest X‑ray Classifier: Normal vs Pneumonia vs Tuberculosis\n",
    "# Jupyter Notebook (drop-in) — Keras/TensorFlow 2.x\n",
    "# -------------------------------------------------------------------\n",
    "# ✅ What this gives you\n",
    "# - 70/15/15 split (train/val/test)\n",
    "# - Preprocessing: resize, normalization, on-the-fly augmentation\n",
    "# - Strong model via transfer learning (EfficientNetV2B0 by default)\n",
    "# - Regularization: Dropout, BatchNorm, Weight Decay (AdamW)\n",
    "# - Metrics: accuracy, precision, recall, F1, AUC (micro/macro), confusion matrix\n",
    "# - Plots: training curves, ROC curves, confusion matrix heatmap\n",
    "# - Explainability: Grad-CAM heatmaps\n",
    "# - Class imbalance handling: class weights (auto-computed)\n",
    "# - Reproducible + clean structure; works with any folder‑structured dataset\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# ========================= 1) SETUP =========================\n",
    "import os, random, math, json, itertools, shutil\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import mixed_precision\n",
    "\n",
    "# Optional: TensorFlow Addons for F1/AdamW. If not present, we will fallback.\n",
    "try:\n",
    "    import tensorflow_addons as tfa\n",
    "    HAS_TFA = True\n",
    "except Exception:\n",
    "    HAS_TFA = False\n",
    "\n",
    "print(tf.__version__, '— TF version')\n",
    "print('TFA available:', HAS_TFA)\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 1337\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "\n",
    "# Mixed precision (speeds up on modern GPUs, safe on CPU too)\n",
    "try:\n",
    "    mixed_precision.set_global_policy('mixed_float16')\n",
    "    MP = True\n",
    "except Exception:\n",
    "    MP = False\n",
    "print('Mixed precision:', MP)\n",
    "\n",
    "# ========================= 2) CONFIG =========================\n",
    "# Point DATA_DIR to a folder that contains *images* inside subfolders named by class, e.g.:\n",
    "# DATA_DIR/\n",
    "#   Normal/\n",
    "#     img1.jpg, img2.jpg, ...\n",
    "#   Pneumonia/\n",
    "#   Tuberculosis/\n",
    "# If you downloaded separate datasets (e.g., one for TB, one for Pneumonia),\n",
    "# place/merge images into these three folders accordingly.\n",
    "\n",
    "DATA_DIR = Path('data/chestxray_3class')  # <-- change to your dataset root folder\n",
    "IMG_SIZE = (256, 256)\n",
    "BATCH_SIZE = 32\n",
    "VAL_SPLIT = 0.15\n",
    "TEST_SPLIT = 0.15\n",
    "TRAIN_SPLIT = 1.0 - VAL_SPLIT - TEST_SPLIT  # 0.70\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "EPOCHS = 20\n",
    "LEARNING_RATE = 3e-4\n",
    "WEIGHT_DECAY = 1e-4  # AdamW weight decay\n",
    "MODEL_NAME = 'EfficientNetV2B0'  # or 'ResNet50', 'EfficientNetB0', etc.\n",
    "SAVE_DIR = Path('artifacts')\n",
    "SAVE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ========================= 3) OPTIONAL: KAGGLE DOWNLOAD =========================\n",
    "# If you want to download via Kaggle API, place kaggle.json in ~/.kaggle/ and uncomment\n",
    "# Example: a placeholder (replace DATASET with an actual Kaggle dataset slug)\n",
    "# !pip install -q kaggle\n",
    "# !kaggle datasets download -d <OWNER/DATASET> -p downloads/ -o\n",
    "# !unzip -q downloads/DATASET.zip -d data/\n",
    "\n",
    "# ========================= 4) COLLECT FILES & SPLIT 70/15/15 =========================\n",
    "assert DATA_DIR.exists(), f\"DATA_DIR not found: {DATA_DIR}\"\n",
    "\n",
    "classes = sorted([d.name for d in DATA_DIR.iterdir() if d.is_dir()])\n",
    "assert set(classes) == set(['Normal','Pneumonia','Tuberculosis']), \\\n",
    "    f\"Expected classes Normal, Pneumonia, Tuberculosis. Found: {classes}\"\n",
    "print('Classes:', classes)\n",
    "\n",
    "# Gather filepaths\n",
    "all_files = []\n",
    "for cls in classes:\n",
    "    for p in (DATA_DIR/cls).glob('**/*'):\n",
    "        if p.suffix.lower() in {'.png','.jpg','.jpeg','.bmp','.tif','.tiff'}:\n",
    "            all_files.append((str(p), cls))\n",
    "\n",
    "df = pd.DataFrame(all_files, columns=['path','label'])\n",
    "print('Total images:', len(df))\n",
    "\n",
    "# Stratified split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, temp_df = train_test_split(\n",
    "    df, test_size=(1.0-TRAIN_SPLIT), stratify=df['label'], random_state=SEED\n",
    ")\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df, test_size=TEST_SPLIT/(TEST_SPLIT+VAL_SPLIT), stratify=temp_df['label'], random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Train: {len(train_df)}  Val: {len(val_df)}  Test: {len(test_df)}\")\n",
    "\n",
    "# ========================= 5) DATASET PIPELINES =========================\n",
    "# Augmentation\n",
    "data_augment = keras.Sequential([\n",
    "    layers.Resizing(IMG_SIZE[0], IMG_SIZE[1]),\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.05),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomContrast(0.1),\n",
    "])\n",
    "\n",
    "# Normalization\n",
    "norm_layer = layers.Rescaling(1./255)\n",
    "\n",
    "# Helper to build tf.data from filepaths\n",
    "def make_ds(paths, labels, training=False):\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    def _load(path, y):\n",
    "        img = tf.io.read_file(path)\n",
    "        img = tf.io.decode_image(img, channels=3, expand_animations=False)\n",
    "        img.set_shape([None, None, 3])\n",
    "        if training:\n",
    "            img = data_augment(img)\n",
    "        else:\n",
    "            img = layers.Resizing(IMG_SIZE[0], IMG_SIZE[1])(img)\n",
    "        img = norm_layer(img)\n",
    "        return img, y\n",
    "    # map, cache, shuffle (if training), batch, prefetch\n",
    "    ds = ds.map(_load, num_parallel_calls=AUTOTUNE)\n",
    "    if training:\n",
    "        ds = ds.shuffle(2048, seed=SEED, reshuffle_each_iteration=True)\n",
    "    ds = ds.batch(BATCH_SIZE).prefetch(AUTOTUNE)\n",
    "    return ds\n",
    "\n",
    "label2idx = {c:i for i,c in enumerate(classes)}\n",
    "idx2label = {i:c for c,i in label2idx.items()}\n",
    "\n",
    "train_paths = train_df['path'].tolist()\n",
    "train_labels = train_df['label'].map(label2idx).astype(np.int32).values\n",
    "val_paths   = val_df['path'].tolist()\n",
    "val_labels  = val_df['label'].map(label2idx).astype(np.int32).values\n",
    "test_paths  = test_df['path'].tolist()\n",
    "test_labels = test_df['label'].map(label2idx).astype(np.int32).values\n",
    "\n",
    "train_ds = make_ds(train_paths, train_labels, training=True)\n",
    "val_ds   = make_ds(val_paths, val_labels, training=False)\n",
    "test_ds  = make_ds(test_paths, test_labels, training=False)\n",
    "\n",
    "# ========================= 6) CLASS WEIGHTS (IMBALANCE) =========================\n",
    "from collections import Counter\n",
    "ctr = Counter(train_labels)\n",
    "class_count = np.array([ctr[i] for i in range(len(classes))])\n",
    "class_weights = {i: (sum(class_count) / (len(classes) * class_count[i])) for i in range(len(classes))}\n",
    "print('Class counts:', dict(ctr))\n",
    "print('Class weights:', class_weights)\n",
    "\n",
    "# ========================= 7) BUILD MODEL (TRANSFER LEARNING) =========================\n",
    "# Switchable backbones\n",
    "BACKBONES = {\n",
    "    'EfficientNetV2B0': keras.applications.efficientnet_v2.EfficientNetV2B0,\n",
    "    'EfficientNetB0': keras.applications.efficientnet.EfficientNetB0,\n",
    "    'ResNet50': keras.applications.resnet50.ResNet50,\n",
    "}\n",
    "assert MODEL_NAME in BACKBONES, f\"Unsupported MODEL_NAME: {MODEL_NAME}\"\n",
    "\n",
    "base = BACKBONES[MODEL_NAME](\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_shape=(IMG_SIZE[0], IMG_SIZE[1], 3)\n",
    ")\n",
    "base.trainable = False  # freeze backbone for warmup\n",
    "\n",
    "inputs = layers.Input(shape=(IMG_SIZE[0], IMG_SIZE[1], 3))\n",
    "x = inputs\n",
    "x = norm_layer(x)  # safety, though already applied in pipeline\n",
    "x = base(x, training=False)\n",
    "x = layers.GlobalAveragePooling2D()(x)\n",
    "x = layers.BatchNormalization()(x)\n",
    "x = layers.Dropout(0.3)(x)\n",
    "outputs = layers.Dense(len(classes), activation='softmax', dtype='float32')(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "\n",
    "# Optimizer with weight decay\n",
    "if HAS_TFA:\n",
    "    optimizer = tfa.optimizers.AdamW(learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
    "else:\n",
    "    optimizer = keras.optimizers.Adam(learning_rate=LEARNING_RATE)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc_ovr', multi_label=False),\n",
    "        keras.metrics.AUC(name='auc_ovo', curve='ROC', multi_label=False)\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Callbacks\n",
    "ckpt_path = str(SAVE_DIR/'best_model.keras')\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(ckpt_path, monitor='val_accuracy', save_best_only=True, mode='max'),\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=2)\n",
    "]\n",
    "\n",
    "# Warmup training (frozen backbone)\n",
    "history1 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=max(3, min(6, EPOCHS//4)),\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fine-tune: unfreeze last N blocks\n",
    "base.trainable = True\n",
    "# Optionally, we can set a fine-tune starting layer for large backbones\n",
    "if MODEL_NAME.startswith('ResNet'):\n",
    "    for layer in base.layers[:-50]:\n",
    "        layer.trainable = False\n",
    "elif MODEL_NAME.startswith('EfficientNet'):\n",
    "    for layer in base.layers[:-50]:\n",
    "        layer.trainable = False\n",
    "\n",
    "if HAS_TFA:\n",
    "    optimizer_ft = tfa.optimizers.AdamW(learning_rate=LEARNING_RATE/5, weight_decay=WEIGHT_DECAY)\n",
    "else:\n",
    "    optimizer_ft = keras.optimizers.Adam(learning_rate=LEARNING_RATE/5)\n",
    "\n",
    "model.compile(\n",
    "    optimizer=optimizer_ft,\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics=[\n",
    "        'accuracy',\n",
    "        keras.metrics.Precision(name='precision'),\n",
    "        keras.metrics.Recall(name='recall'),\n",
    "        keras.metrics.AUC(name='auc_macro', multi_label=True, num_labels=len(classes), average='macro'),\n",
    "        keras.metrics.AUC(name='auc_micro', multi_label=True, num_labels=len(classes), average='micro')\n",
    "    ]\n",
    ")\n",
    "\n",
    "history2 = model.fit(\n",
    "    train_ds,\n",
    "    validation_data=val_ds,\n",
    "    epochs=EPOCHS,\n",
    "    class_weight=class_weights,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# ========================= 8) TRAINING CURVES =========================\n",
    "def plot_history(hlist, key, title=None):\n",
    "    plt.figure()\n",
    "    vals = []\n",
    "    for h in hlist:\n",
    "        vals += h.history.get(key, [])\n",
    "    plt.plot(vals)\n",
    "    plt.title(title or key)\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel(key)\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "plot_history([history1, history2], 'loss', 'Training Loss')\n",
    "plot_history([history1, history2], 'val_loss', 'Validation Loss')\n",
    "plot_history([history1, history2], 'accuracy', 'Training Accuracy')\n",
    "plot_history([history1, history2], 'val_accuracy', 'Validation Accuracy')\n",
    "\n",
    "# ========================= 9) EVALUATION: METRICS & CONFUSION MATRIX =========================\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "\n",
    "# Predict\n",
    "y_true = test_labels\n",
    "probs = model.predict(test_ds, verbose=1)\n",
    "y_pred = np.argmax(probs, axis=1)\n",
    "\n",
    "# Classification report (precision, recall, f1 per class)\n",
    "report = classification_report(y_true, y_pred, target_names=classes, output_dict=True)\n",
    "print(pd.DataFrame(report).T)\n",
    "\n",
    "# Save report\n",
    "pd.DataFrame(report).T.to_csv(SAVE_DIR/'classification_report.csv')\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "fig = plt.figure()\n",
    "plt.imshow(cm, interpolation='nearest')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(classes))\n",
    "plt.xticks(tick_marks, classes, rotation=45)\n",
    "plt.yticks(tick_marks, classes)\n",
    "thresh = cm.max() / 2.\n",
    "for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "    plt.text(j, i, format(cm[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ROC & AUC (one-vs-rest)\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "y_true_bin = label_binarize(y_true, classes=list(range(len(classes))))\n",
    "fig = plt.figure()\n",
    "for i, cls in enumerate(classes):\n",
    "    fpr, tpr, _ = roc_curve(y_true_bin[:, i], probs[:, i])\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.plot(fpr, tpr, label=f\"{cls} (AUC = {roc_auc:.3f})\")\n",
    "plt.plot([0,1],[0,1],'--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curves (OvR)')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# ========================= 10) SAVE MODEL & LABEL MAP =========================\n",
    "model.save(SAVE_DIR/'final_model.keras')\n",
    "with open(SAVE_DIR/'labels.json','w') as f:\n",
    "    json.dump(idx2label, f)\n",
    "print('Saved to', SAVE_DIR)\n",
    "\n",
    "# ========================= 11) GRAD-CAM FOR EXPLAINABILITY =========================\n",
    "# Utility functions adapted for Keras models\n",
    "\n",
    "def get_last_conv_layer(model):\n",
    "    # Find the last Conv2D layer\n",
    "    for layer in reversed(model.layers):\n",
    "        if isinstance(layer, layers.Conv2D) or 'conv' in layer.name.lower():\n",
    "            return layer.name\n",
    "    # If the backbone is nested, try inside\n",
    "    for layer in reversed(model.layers):\n",
    "        if hasattr(layer, 'layers'):\n",
    "            for l2 in reversed(layer.layers):\n",
    "                if isinstance(l2, layers.Conv2D) or 'conv' in l2.name.lower():\n",
    "                    return l2.name\n",
    "    raise ValueError('No conv layer found for Grad-CAM')\n",
    "\n",
    "last_conv_name = get_last_conv_layer(model)\n",
    "print('Grad-CAM last conv layer:', last_conv_name)\n",
    "\n",
    "@tf.function\n",
    "def grad_cam(img_tensor, class_index=None):\n",
    "    grad_model = keras.models.Model([model.inputs], [model.get_layer(last_conv_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_outputs, predictions = grad_model(img_tensor)\n",
    "        if class_index is None:\n",
    "            class_index = tf.argmax(predictions[0])\n",
    "        loss = predictions[:, class_index]\n",
    "    grads = tape.gradient(loss, conv_outputs)\n",
    "    pooled_grads = tf.reduce_mean(grads, axis=(0,1,2))\n",
    "    conv_outputs = conv_outputs[0]\n",
    "    heatmap = tf.reduce_sum(tf.multiply(pooled_grads, conv_outputs), axis=-1)\n",
    "    heatmap = tf.maximum(heatmap, 0) / (tf.reduce_max(heatmap) + 1e-8)\n",
    "    heatmap = tf.image.resize(heatmap[..., None], IMG_SIZE)\n",
    "    heatmap = tf.squeeze(heatmap)\n",
    "    return heatmap.numpy()\n",
    "\n",
    "# Visualize a few test images\n",
    "sample_idx = np.random.choice(len(test_paths), size=min(6,len(test_paths)), replace=False)\n",
    "for idx in sample_idx:\n",
    "    path = test_paths[idx]\n",
    "    label = idx2label[int(test_labels[idx])]\n",
    "    raw = tf.io.read_file(path)\n",
    "    img = tf.io.decode_image(raw, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    inp = norm_layer(tf.expand_dims(tf.cast(img, tf.float32), 0))\n",
    "    prob = model.predict(inp, verbose=0)[0]\n",
    "    pred_idx = int(np.argmax(prob))\n",
    "    pred_label = idx2label[pred_idx]\n",
    "    heatmap = grad_cam(inp, pred_idx)\n",
    "\n",
    "    plt.figure()\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.title(f\"True: {label}\\nPred: {pred_label} ({prob[pred_idx]:.2f})\")\n",
    "    plt.imshow(tf.cast(img, tf.uint8))\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.title('Grad-CAM')\n",
    "    plt.imshow(tf.cast(img, tf.uint8))\n",
    "    plt.imshow(heatmap, alpha=0.35)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "# ========================= 12) ETHICS, BIAS & DEPLOYMENT NOTES =========================\n",
    "print('\\nNOTES — Ethics & Fairness:')\n",
    "print('- Validate on an external test set from a different hospital to check generalization.')\n",
    "print('- Ensure class balance when reporting metrics; show per-class sensitivity (recall).')\n",
    "print('- Check performance across demographics (age/sex/region if available) to detect bias.')\n",
    "print('- This model is a decision support tool, NOT a diagnostic device. Keep a clinician-in-the-loop.')\n",
    "\n",
    "# ========================= 13) INFERENCE SNIPPET (LOADING SAVED MODEL) =========================\n",
    "# Example function to load and predict on a single image\n",
    "\n",
    "def load_artifacts(model_path=SAVE_DIR/'final_model.keras', labels_path=SAVE_DIR/'labels.json'):\n",
    "    mdl = keras.models.load_model(model_path)\n",
    "    with open(labels_path,'r') as f:\n",
    "        id2lbl = json.load(f)\n",
    "    return mdl, {int(k):v for k,v in id2lbl.items()}\n",
    "\n",
    "\n",
    "def predict_image(image_path):\n",
    "    mdl, id2lbl = load_artifacts()\n",
    "    raw = tf.io.read_file(image_path)\n",
    "    img = tf.io.decode_image(raw, channels=3, expand_animations=False)\n",
    "    img = tf.image.resize(img, IMG_SIZE)\n",
    "    img = tf.expand_dims(img, 0)\n",
    "    img = norm_layer(img)\n",
    "    prob = mdl.predict(img, verbose=0)[0]\n",
    "    idx = int(np.argmax(prob))\n",
    "    return id2lbl[idx], float(prob[idx])\n",
    "\n",
    "# Example:\n",
    "# pred, conf = predict_image('some_image.jpg')\n",
    "# print('Prediction:', pred, 'Confidence:', conf)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
